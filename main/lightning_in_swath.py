'''
INPUT:
    - S5P (TROPOPMI) L2 product files
    - ENGLN lightning flash daily csv files or daily VAISALA lightning density/flash csv files
    - S5P swath shape file generated by fetch_s5p_portal_geometry.py
        https://github.com/zxdawn/weather_data/blob/master/S5P-PAL/fetch_s5p_portal_geometry.py

OUTPUT:
    csv file (timsestamp, longitude, latitude)

UPDATE:
    Xin Zhang:
        2023-01-11: basic version
'''


import os
from glob import glob
import logging
import geopandas as gpd
import pandas as pd
from satpy import Scene
from datetime import timedelta
from s5p_lno2_utils import Config
from shapely.geometry import Point
from multiprocessing import Pool


# Choose the following line for info or debugging:
logging.basicConfig(level=logging.INFO)
# logging.basicConfig(level=logging.DEBUG)
logging.getLogger('satpy').setLevel(logging.ERROR)


def load_s5p(filename):
    """Read TROPOMI data"""
    scn = Scene([filename], reader='tropomi_l2')
    scn.load(['time_utc', 'longitude', 'latitude'])

    overpass_time = pd.to_datetime(scn['time_utc'].where(scn['latitude'].mean('x') >= lat_min, drop=True)).mean()

    return scn, overpass_time

def load_lightning(scn, t_overpass):
    """Read lightning data"""
    day_now = t_overpass
    day_pre = day_now-timedelta(days=1)

    lightning_list = [day.strftime(f"{cfg['lightning_dir']}/%Y%m/%Y%m%d.csv") for day in [day_pre, day_now]]
    # drop not existed filename
    lightning_list = [filename for filename in lightning_list if os.path.exists(filename)]

    # read lightning and VIIRS data
    logging.debug(f'    Reading {lightning_list} ...')
    df_lightning = pd.concat(map(pd.read_csv, lightning_list))
    df_lightning = df_lightning[df_lightning.latitude >= lat_min]

    # get lightning dots during the several hours before the mean overpass time
    df_lightning['timestamp'] = pd.to_datetime(df_lightning['timestamp'], utc=True)
    delta = df_lightning['timestamp'] - t_overpass
    df_lightning['delta'] = delta.dt.total_seconds()/60

    # pick lightning happened during 3 hours before overpass
    subset = (-180 < df_lightning['delta']) & (df_lightning['delta'] < 0)
    df_lightning = df_lightning[subset]

    return df_lightning[['timestamp', 'longitude', 'latitude']]


def point_in_swath(df_lightning, polys):
    '''Pick lightning inside swath'''
    points = df_lightning.apply(lambda row: Point(row['longitude'],row['latitude']),axis=1)
    mask = gpd.GeoSeries(points).within(polys)

    return df_lightning[mask]

def process_data(filepath):
    """Process TROPOMI and lightning data"""
    filename = os.path.basename(filepath)
    polys = gdf[gdf.filename == filename]['geometry'].item()

    # load tropomi data
    logging.info(f'    Processing {filename} ...')
    scn, t_overpass = load_s5p(filepath)

    if pd.isnull(t_overpass):
        t_overpass = pd.to_datetime(scn['longitude'].attrs['end_time'], utc=True)

    # load lightning data
    logging.debug('    Reading lightning data ...')
    df_lightning = load_lightning(scn, t_overpass)

    # get lightning points within the subset of tropomi swath
    logging.debug('    Pick the lightning within swath ...')
    df_subset = point_in_swath(df_lightning, polys)

    # remove utc
    df_subset['timestamp'] = df_subset['timestamp'].dt.tz_localize(None)

    return df_subset


def main():
    # get all filenames based on requested date range
    pattern = os.path.join(s5p_dir, '{}{:02}', 'S5P_*_L2__NO2____{}{:02}{:02}T*')
    filelist = sum([glob(pattern.format(date.year, date.month, date.year, date.month, date.day)) for date in req_dates], [])
    filenames = [os.path.basename(file) for file in filelist]


    with Pool(num_pool) as pool:
        res = pool.map(process_data, filelist)

    # for filepath in filelist:
    #     df = process_data(gdf, filepath)

    # merge and drop duplicated lightning
    output = pd.concat(res, axis=0).drop_duplicates().sort_values('timestamp')

    # export file
    st = req_dates[0].strftime('%Y%m%d')
    et = req_dates[-1].strftime('%Y%m%d')

    savename = cfg['output_data_dir'] + f'/swath_lightning_' + st + '_' + et + '.csv'
    logging.info(f'export to {savename}')

    output.to_csv(savename, index=False)


if __name__ == '__main__':
    # read config file
    cfg = Config('settings.txt')
    s5p_dir = cfg['s5p_dir']
    lat_min = 60
    num_pool = 4

    # generate data range
    req_dates = pd.date_range(start=cfg['start_date'],
                              end=cfg['end_date'],
                              freq='D')

    # read swaths shape fil
    gdf = gpd.read_file(s5p_dir+'/s5p_swaths.shp')

    main()
